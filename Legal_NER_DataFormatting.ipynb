{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdglLPBcInjzVYSHrayqNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsal-kr/LegalNER-CS60075/blob/main/Legal_NER_DataFormatting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shhfwc-86Vgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb123e26-c9e2-4cfc-825e-040ab1d1f53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import nltk\n",
        "import bisect\n",
        "import re\n",
        "import spacy\n",
        "from collections import Counter, defaultdict\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "nltk.download('punkt')\n",
        "!python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "# sys.path.append('/content/drive/My Drive/LegalEval/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8j1rdTqcr6C",
        "outputId": "f9ba81f3-e741-4ef1-de2c-b9f88ee9b1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-02-21 19:21:32.480313: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 19:21:32.480461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-21 19:21:32.480487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-02-21 19:21:34.005774: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/My Drive/LegalEval/NER_TRAIN.zip'\n",
        "!unzip '/content/drive/My Drive/LegalEval/NER_DEV.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ElTRE4udfRJ",
        "outputId": "37d461ad-e128-4a89-8b01-9ed6af31b782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/LegalEval/NER_TRAIN.zip\n",
            "  inflating: NER_TRAIN_JUDGEMENT.json  \n",
            "  inflating: NER_TRAIN_PREAMBLE.json  \n",
            "Archive:  /content/drive/My Drive/LegalEval/NER_DEV.zip\n",
            "   creating: NER_DEV/\n",
            "  inflating: NER_DEV/.DS_Store       \n",
            "  inflating: __MACOSX/NER_DEV/._.DS_Store  \n",
            "  inflating: NER_DEV/NER_DEV_PREAMBLE.json  \n",
            "   creating: NER_DEV/.ipynb_checkpoints/\n",
            "  inflating: NER_DEV/NER_DEV_JUDGEMENT.json  \n",
            "  inflating: NER_DEV/.ipynb_checkpoints/CYCLE1_FINAL_REVIEW_COMBINED_29_AUG-checkpoint.json  \n",
            "  inflating: NER_DEV/.ipynb_checkpoints/CYCLE1_FINAL_REVIEW_JUDGEMENT_29_AUG-checkpoint.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_df = pd.read_json('NER_TRAIN_JUDGEMENT.json')\n",
        "preamble_df = pd.read_json('NER_TRAIN_PREAMBLE.json')"
      ],
      "metadata": {
        "id": "AG1MrmcKdrnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_texts = judgements_df['data'].str.get('text')\n",
        "preamble_texts = preamble_df['data'].str.get('text')"
      ],
      "metadata": {
        "id": "5HxqzkAVf6RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_labels = judgements_df['annotations'].str[0].str.get('result')\n",
        "judgement_labels = [[label.get('value') for label in all_labels] for all_labels in judgement_labels]\n",
        "preamble_labels = preamble_df['annotations'].str[0].str.get('result')\n",
        "preamble_labels = [[label.get('value') for label in all_labels] for all_labels in preamble_labels]"
      ],
      "metadata": {
        "id": "WY1N9h36hVL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenized_text_and_labels(texts, labels):\n",
        "  tokenized_texts, bio_labels = [], []\n",
        "  for text, label in zip(texts, labels):\n",
        "    entities = []\n",
        "    for each_label in label:\n",
        "      entities.append((each_label['start'], each_label['end'], each_label['labels'][0]))\n",
        "    doc = nlp(text)\n",
        "    bio_labels.append(offsets_to_biluo_tags(doc, entities))\n",
        "\n",
        "    tokenized_texts.append([x.text for x in doc])\n",
        "\n",
        "  return tokenized_texts, bio_labels"
      ],
      "metadata": {
        "id": "YMkjEkWwiVbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_texts_tokenized, judgement_labels_tagged = generate_tokenized_text_and_labels(judgement_texts, judgement_labels)\n",
        "preamble_texts_tokenized, preamble_labels_tagged = generate_tokenized_text_and_labels(preamble_texts, preamble_labels)"
      ],
      "metadata": {
        "id": "HtCD18Qbi_75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data = pd.concat([pd.Series(judgement_texts, name='text'), pd.Series(judgement_texts_tokenized, name='text_tokenized'), pd.Series(judgement_labels_tagged, name='label')], axis=1)\n",
        "preamble_data = pd.concat([pd.Series(preamble_texts, name='text'), pd.Series(preamble_texts_tokenized, name='text_tokenized'), pd.Series(preamble_labels_tagged, name='label')], axis=1)"
      ],
      "metadata": {
        "id": "9ocZ544qn8Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data['label'] = judgements_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[U*L*]+-', 'I-', x), lst)))\n",
        "preamble_data['label'] = preamble_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[U*L*]+-', 'I-', x), lst)))"
      ],
      "metadata": {
        "id": "bq75hx_PfxU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data['label'] = judgements_data['label'].apply(lambda lst: list(map(lambda x: 'O' if x=='-' else x, lst)))\n",
        "preamble_data['label'] = preamble_data['label'].apply(lambda lst: list(map(lambda x: 'O' if x=='-' else x, lst)))"
      ],
      "metadata": {
        "id": "4V3trdv4bcD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# judgements_data['label'] = judgements_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[\\([{\\'})\\]]', '', x), lst)))\n",
        "# preamble_data['label'] = preamble_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[\\([{\\'})\\]]', '', x), lst)))"
      ],
      "metadata": {
        "id": "Enlt7pzGT93E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data.to_pickle('/content/drive/My Drive/LegalEval/judgement_data_train.pkl')\n",
        "preamble_data.to_pickle('/content/drive/My Drive/LegalEval/preamble_data_train.pkl')"
      ],
      "metadata": {
        "id": "5abkFNZGsSwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Vocabulary of words and tags"
      ],
      "metadata": {
        "id": "UuWGPJgZPUNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_count_word = 1\n",
        "min_count_tag = 1\n",
        "pad_word = '<pad>'\n",
        "pad_tag = 'O'\n",
        "unk_word = 'UNK'\n",
        "words_vocab, tags_vocab = Counter(), Counter()\n",
        "\n",
        "text_lines = list(judgements_data['text'])\n",
        "for line in text_lines:\n",
        "  words_vocab.update(line.strip().split(' '))\n",
        "text_lines = list(preamble_data['text'])\n",
        "for line in text_lines:\n",
        "  words_vocab.update(line.strip().split(' '))\n",
        "\n",
        "label_lines = [' '.join(judgements_data['label'][i]) for i in range(len(judgements_data))]\n",
        "for line in label_lines:\n",
        "  tags_vocab.update(line.strip().split(' '))\n",
        "label_lines = [' '.join(preamble_data['label'][i]) for i in range(len(preamble_data))]\n",
        "for line in label_lines:\n",
        "  tags_vocab.update(line.strip().split(' '))"
      ],
      "metadata": {
        "id": "nKJOq9YnnHyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [tok for tok, count in words_vocab.items() if count >= min_count_word]\n",
        "tags = [tok for tok, count in tags_vocab.items() if count >= min_count_tag]\n",
        "\n",
        "# Add pad tokens\n",
        "if pad_word not in words: words.append(pad_word)\n",
        "if pad_tag not in tags: tags.append(pad_tag)\n",
        "\n",
        "# add word for unknown words \n",
        "words.append(unk_word)"
      ],
      "metadata": {
        "id": "1EbP-R-SEV9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/LegalEval/words.txt', \"w\") as f:\n",
        "  for token in words:\n",
        "    f.write(token + '\\n')\n",
        "with open('/content/drive/My Drive/LegalEval/tags.txt', \"w\") as f:\n",
        "  for token in tags:\n",
        "    f.write(token + '\\n')"
      ],
      "metadata": {
        "id": "w6Vy__oeI2RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dev Data"
      ],
      "metadata": {
        "id": "W1A3WLhVSgFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_df = pd.read_json('NER_DEV/NER_DEV_JUDGEMENT.json')\n",
        "preamble_df = pd.read_json('NER_DEV/NER_DEV_PREAMBLE.json')"
      ],
      "metadata": {
        "id": "ySAiEc2gSlBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_texts = judgements_df['data'].str.get('text')\n",
        "preamble_texts = preamble_df['data'].str.get('text')"
      ],
      "metadata": {
        "id": "LuZQATlNSlBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_labels = judgements_df['annotations'].str[0].str.get('result')\n",
        "judgement_labels = [[label.get('value') for label in all_labels] for all_labels in judgement_labels]\n",
        "preamble_labels = preamble_df['annotations'].str[0].str.get('result')\n",
        "preamble_labels = [[label.get('value') for label in all_labels] for all_labels in preamble_labels]"
      ],
      "metadata": {
        "id": "xur5vioISlBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_tokenized_text_and_labels(texts, labels):\n",
        "  tokenized_texts, bio_labels = [], []\n",
        "  for text, label in zip(texts, labels):\n",
        "    entities = []\n",
        "    for each_label in label:\n",
        "      entities.append((each_label['start'], each_label['end'], each_label['labels'][0]))\n",
        "    doc = nlp(text)\n",
        "    bio_labels.append(offsets_to_biluo_tags(doc, entities))\n",
        "\n",
        "    tokenized_texts.append([x.text for x in doc])\n",
        "\n",
        "  return tokenized_texts, bio_labels"
      ],
      "metadata": {
        "id": "LFzZMJInSlBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgement_texts_tokenized, judgement_labels_tagged = generate_tokenized_text_and_labels(judgement_texts, judgement_labels)\n",
        "preamble_texts_tokenized, preamble_labels_tagged = generate_tokenized_text_and_labels(preamble_texts, preamble_labels)"
      ],
      "metadata": {
        "id": "55GbDXiaSlBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8e508d-4e0f-4510-aec7-25c40b9af5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"This will be clarified in the instant case by comp...\" with entities \"[(60, 127, 'PROVISION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "\n",
            "Digitally signed by:RAJENDER SINGH KARKI Signing...\" with entities \"[(22, 42, 'OTHER_PERSON'), (56, 66, 'DATE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"It is seen from the record that the arbitration pr...\" with entities \"[(88, 99, 'OTHER_PERSON'), (171, 180, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"It is in that context that the observations made b...\" with entities \"[(78, 85, 'OTHER_PERSON'), (205, 329, 'PRECEDENT')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"68 Marata(J) final.doc available in the Sate of Ma...\" with entities \"[(48, 59, 'GPE'), (160, 170, 'COURT'), (193, 228, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"(Manohar Lal Sharma14; Committee for Protection of...\" with entities \"[(1, 19, 'OTHER_PERSON'), (23, 68, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The said judgment was reported in M. Venkateswara ...\" with entities \"[(32, 96, 'PRECEDENT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            " Likewise, In the power of attorney (exhibit P/11...\" with entities \"[(70, 84, 'OTHER_PERSON'), (98, 109, 'OTHER_PERSON...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "\n",
            " (c) The aforesaid cheque was presented by the c...\" with entities \"[(93, 104, 'ORG'), (118, 127, 'GPE'), (226, 236, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"We may also point out that on an earlier occasion ...\" with entities \"[(92, 121, 'CASE_NUMBER'), (145, 173, 'PRECEDENT')...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The respondent M/s. Ultratech Cement Ltd. (hereina...\" with entities \"[(20, 41, 'RESPONDENT'), (179, 191, 'OTHER_PERSON'...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"It is further urged that the aforesaid three decis...\" with entities \"[(113, 126, 'COURT'), (190, 209, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
            "    Commercial Complex, Raj Bhavan Road, Hyderaba...\" with entities \"[(42, 51, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"There is no mechanism for Patna High Court REQ. CA...\" with entities \"[(26, 42, 'COURT'), (48, 68, 'CASE_NUMBER'), (72, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Mr. Arun Bharadwaj, ld. CGSC, appearing for the Un...\" with entities \"[(4, 18, 'OTHER_PERSON'), (48, 62, 'ORG'), (111, 1...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad\n",
            " \n",
            " \n",
            "\n",
            "       ...\" with entities \"[(0, 37, 'COURT'), (315, 354, 'PETITIONER'), (371,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court For The State Of Telangana\n",
            "     ...\" with entities \"[(7, 45, 'COURT'), (190, 201, 'JUDGE'), (330, 348,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court Of Judicature At Madras\n",
            "\n",
            "       ...\" with entities \"[(7, 41, 'COURT'), (232, 244, 'JUDGE'), (342, 432,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"In The High Court Of Karnataka At Bengaluru\n",
            "\n",
            "    D...\" with entities \"[(7, 43, 'COURT'), (138, 159, 'JUDGE'), (291, 307,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Honourable Dr. Justice B.Siva Sankara Rao         ...\" with entities \"[(23, 41, 'JUDGE'), (103, 125, 'PETITIONER'), (174...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"W.P.No.9267 of 2017\n",
            "\n",
            "                             ...\" with entities \"[(63, 97, 'COURT'), (295, 306, 'JUDGE'), (426, 444...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"1\n",
            "\n",
            "\n",
            "                                              ...\" with entities \"[(120, 142, 'COURT'), (269, 283, 'PETITIONER'), (4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"C/Lpa/1655/2019                             Judgme...\" with entities \"[(95, 129, 'COURT'), (466, 479, 'JUDGE'), (545, 56...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Non­Reportable\n",
            "                                 In...\" with entities \"[(55, 77, 'COURT'), (218, 236, 'PETITIONER'), (339...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad\n",
            " \n",
            " \n",
            "\n",
            "Afr\n",
            " \n",
            "R...\" with entities \"[(0, 37, 'COURT'), (130, 168, 'PETITIONER'), (197,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"1\n",
            "\n",
            "                            Before The Madurai ...\" with entities \"[(42, 76, 'COURT'), (268, 280, 'JUDGE'), (543, 573...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"High Court Of Judicature At Allahabad, Lucknow Ben...\" with entities \"[(0, 52, 'COURT'), (175, 184, 'PETITIONER'), (213,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data = pd.concat([pd.Series(judgement_texts, name='text'), pd.Series(judgement_texts_tokenized, name='text_tokenized'), pd.Series(judgement_labels_tagged, name='label')], axis=1)\n",
        "preamble_data = pd.concat([pd.Series(preamble_texts, name='text'), pd.Series(preamble_texts_tokenized, name='text_tokenized'), pd.Series(preamble_labels_tagged, name='label')], axis=1)"
      ],
      "metadata": {
        "id": "g1ABV2NoSlBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data['label'] = judgements_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[U*L*]+-', 'I-', x), lst)))\n",
        "preamble_data['label'] = preamble_data['label'].apply(lambda lst: list(map(lambda x: re.sub('[U*L*]+-', 'I-', x), lst)))"
      ],
      "metadata": {
        "id": "XfOU0bnXSlBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data['label'] = judgements_data['label'].apply(lambda lst: list(map(lambda x: 'O' if x=='-' else x, lst)))\n",
        "preamble_data['label'] = preamble_data['label'].apply(lambda lst: list(map(lambda x: 'O' if x=='-' else x, lst)))"
      ],
      "metadata": {
        "id": "XUsIqjpeULZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judgements_data.to_pickle('/content/drive/My Drive/LegalEval/judgement_data_dev.pkl')\n",
        "preamble_data.to_pickle('/content/drive/My Drive/LegalEval/preamble_data_dev.pkl')"
      ],
      "metadata": {
        "id": "eCmnnjOmSlBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# judgements_data.iloc[16]['label']"
      ],
      "metadata": {
        "id": "8D9rHcitV5Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# judgement_labels[37]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MrRgOp5bZEt",
        "outputId": "3072307f-9303-45fd-bb8e-0754802d52bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'start': 22,\n",
              "  'end': 42,\n",
              "  'text': 'RAJENDER SINGH KARKI',\n",
              "  'labels': ['OTHER_PERSON']},\n",
              " {'start': 56, 'end': 66, 'text': '13.01.2021', 'labels': ['DATE']}]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for x in nlp(judgement_texts[37]):\n",
        "#   print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrSldlbjfxM0",
        "outputId": "6a164ca2-8c65-44d9-dcc5-c2f41c88ea90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Digitally\n",
            "signed\n",
            "by\n",
            ":\n",
            "RAJENDER\n",
            "SINGH\n",
            "KARKI\n",
            "Signing\n",
            "Date:13.01.2021\n",
            "18:54:39\n",
            " \n",
            "31\n",
            ".\n",
            "Claimant(s\n",
            ")\n",
            "to\n",
            "respond\n",
            "to\n",
            "the\n",
            "offer\n",
            "of\n",
            "the\n",
            "Insurance\n",
            "Company\n",
            "within\n",
            "30\n",
            "days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = judgement_texts[16]\n",
        "# doc = nlp(text)\n",
        "# entities = [(60, 127, \"['PROVISION']\")]\n",
        "# offsets_to_biluo_tags(nlp.make_doc(text), entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlOVCFjFcUIB",
        "outputId": "287ba591-8e52-4068-bdbc-0e953db437ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " 'O',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zbUV38yqfT1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}